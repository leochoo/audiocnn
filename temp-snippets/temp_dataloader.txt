from torch.utils.data import DataLoader, Dataset, random_split
import torchaudio


def static_preprocessing(Dataset):

    # append the new column
    df append["preprocessed"]

    # all these can move into processed_dataset
    # Absolute file path of the audio file - concatenate the audio directory with

    # append to the dataframe

    for each file:
        # the relative path
        audio_file = self.data_path + self.df.loc[idx, 'relative_path']
        # Get the Class ID
        class_id = self.df.loc[idx, 'classID']
        # static operation - needs to be run only once
        AudioUtil.open(audio_file):
            return (sig, sr)
        AudioUtil.resample(aud, newsr):
        AudioUtil.rechannel(aud, new_channel):
        AudioUtil.pad_trunc(aud, max_ms):

            # add the data to the preprocessed column
        df[idx, "preprocessed"].append(preprocessed_audio)

    return processed_dataset


# ----------------------------
# Sound Dataset
# ----------------------------


class SoundDS(Dataset):
    def __init__(self, df):
        self.df = df
        self.duration = 4000
        self.sr = 44100
        self.channel = 2
        self.shift_pct = 0.4

    # ----------------------------
    # Number of items in dataset
    # ----------------------------
    def __len__(self):
        return len(self.df)

    # ----------------------------
    # Get i'th item in dataset
    # ----------------------------
    def __getitem__(self, idx):

        # dynamic shori
        preprocessed_audio = self.df.loc[idx, 'preprocessed']
        class_id = self.df.loc[idx, 'classID']

        shift_aud = AudioUtil.time_shift(preprocessed_audio, self.shift_pct)
        sgram = AudioUtil.spectro_gram(
            shift_aud, n_mels=64, n_fft=1024, hop_len=None)
        aug_sgram = AudioUtil.spectro_augment(
            sgram, max_mask_pct=0.1, n_freq_masks=2, n_time_masks=2)

        return aug_sgram, class_id
